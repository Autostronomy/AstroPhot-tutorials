{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman, Rubin SN Simulation modeling with AstroPhot\n",
    "\n",
    "Author: Michael Wood-Vasey <wmwv@pitt.edu>  \n",
    "Last Verified to run: 2023-11-09\n",
    "\n",
    "Use the [AstroPhot](https://autostronomy.github.io/AstroPhot/) package to model lightcurve of SN in Roman HLTDS [Supernova Survey](https://ui.adsabs.harvard.edu/abs/2023MNRAS.523.3874W)\n",
    "or Rubin LSST [LSST DESC DC2](https://data.lsstdesc.org/doc/dc2_sim_sky_survey) simulations.\n",
    "\n",
    "Notable Requirements:  \n",
    "astrophot  \n",
    "astropy  \n",
    "torch  \n",
    "\n",
    "Major TODOs:\n",
    "  * [~] Get reasonable PSF model for image\n",
    "    - Done for DC2\n",
    "  * [ ] Get calibrated zeropoints and make sure they're correct for the given PSF (i.e., aperture corrections).\n",
    "  * [~] Implement SIP WCS in AstroPhot to deal with slight variation in object positions\n",
    "    - Instead implemented a per-image (but not per object) astrometric shift.\n",
    "  * [x] Update to v0.13 AstroPhot constraints.\n",
    "  * [ ] Fit in linear flux\n",
    "  * [ ] Actually use a GPU (start with NERSC, hope for Apple Silicon MPS development)\n",
    "  * [ ] Compare to truth catalog\n",
    "  * [ ] SED Modeling\n",
    "  * [ ] Mock up transmission function\n",
    "  * [ ] Integrate with SN lightcurve model (e.g., SALT3 or friends)\n",
    "  * [ ] Come up with galaxy SED model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This tutorial presents two options for datasets: \"DC2\" and \"RomanSN\".  The datasets were chosen because they were of interest to the primary author (MWV).  Unfortunately, there's just a bit of work to download the data.  For the Roman SN simulations, the extra work is that you need to download the full image focal planes, even when we're just using one detector from each (out of 18 total).  For the Rubin LSST DESC DC2 simulations, the extra work is using either Globus to download the data from the [DESC Data Archive](https://data.lsstdesc.org/doc/download), or using the [Rubin Science Pipelines DP0.2](https://data.lsst.cloud) to download the data through the Portal or API interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select `DATASET` as either \"DC2\" or \"RomanSN\"\n",
    "DATASET = \"DC2\"\n",
    "# DATASET = \"RomanSN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSST DESC Data Challenge 2 Data\n",
    "\n",
    "The first data set use data from the LSST DESC DC2 simulated data set as processed by the LSST Science Pipelines for Data Preview 0.2.\n",
    "\n",
    "https://arxiv.org/abs/2101.04855  \n",
    "https://ui.adsabs.harvard.edu/abs/2021ApJS..253...31L.\n",
    "\n",
    "\n",
    "Option 1: Access to the DESC DC2 through the [DESC Data Archive](https://data.lsstdesc.org/doc/download) requires creating a Globus account and having a Globus end point whereever you want to put the data.  Then GUI selection of the datasets you want to download.\n",
    "\n",
    "Option 2: Access to the Rubin-processed DP0.2 data requires [registering to be a DP0 Delegate](https://dp0-2.lsst.io/dp0-delegate-resources/index.html#) and being an LSST Data Rights Holder.\n",
    "\n",
    "This tutorial was written using data that were downloaded through the https://data.lsst.cloud Portal as the g-, r-, and i-band images overlapping the position of a supernova simulated in the DC2 data: ICRS (RA, Dec): (60.2901401, -44.142051) degrees during the 2025-08-01 through 2025-12-31.\n",
    "\n",
    "This tutorials assumes that the DC2 image files will be placed into:\n",
    "DATADIR = \"data/DC2\"\n",
    "\n",
    "The DC2 tables of simulated SNe, can be downloaded via Globus at:\n",
    "\n",
    "https://data.lsstdesc.org/browse/dataset/4dab60f0-1b22-4304-b01d-519311783c4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roman HLTDS Data\n",
    "\n",
    "The other data set uses data from Roman simulations of SN in \n",
    "\"A synthetic Roman Space Telescope High-Latitude Time-Domain Survey: supernovae in the deep field\"\n",
    "Wang et al. 2023, MNRAS, 523, 3, 3874.\n",
    "https://ui.adsabs.harvard.edu/abs/2023MNRAS.523.3874W\n",
    "\n",
    "Data page\n",
    "https://roman.ipac.caltech.edu/sims/SN_Survey_Image_sim.html\n",
    "\n",
    "We're here trying out 3 images selected by Lauren Aldoroty that have a SN.  You could download them here with the following bash script: \n",
    "\n",
    "Note that this scripts work, but gets you much more data than we needed. The three images are 1.3 GB each, covering the 18 detectors for the Roman WFI instrument.  We only need one, but the tarballs are for the full focal plane.  The rotate_Y_truth catalog is 288 MB.  The remaining catalog files are much smaller.\n",
    "\n",
    "```\n",
    "DATADIR=data/RomanSN\n",
    "mkdir -p ${DATADIR}\n",
    "curl \"https://roman.ipac.caltech.edu/data/sims/sn_image_sims/rotate_update_Y106_132.tar.gz\" --output ${DATADIR}/rotate_update_Y106_132.tar.gz\n",
    "curl \"https://roman.ipac.caltech.edu/data/sims/sn_image_sims/rotate_update_Y106_174.tar.gz\" --output ${DATADIR}/rotate_update_Y106_174.tar.gz\n",
    "curl \"https://roman.ipac.caltech.edu/data/sims/sn_image_sims/rotate_update_Y106_175.tar.gz\" --output ${DATADIR}/rotate_update_Y106_175.tar.gz\n",
    "\n",
    "# SN input catalog / SN\n",
    "curl https://roman.ipac.caltech.edu/data/sims/sn_image_sims/WFIRST_AKARI_FIXED_HEAD.FITS --output ${DATADIR}/WFIRST_AKARI_FIXED_HEAD.FITS\n",
    "\n",
    "# SN input lightcurves\n",
    "curl https://roman.ipac.caltech.edu/data/sims/sn_image_sims/WFIRST_AKARI_FIXED_PHOT.FITS.gz --output ${DATADIR}/WFIRST_AKARI_FIXED_PHOT.FITS.gz\n",
    "\n",
    "# SN truth\n",
    "curl https://roman.ipac.caltech.edu/data/sims/sn_image_sims/rotate_Y_truth.tar.gz --output ${DATADIR}/rotate_Y_truth.tar.gz\n",
    "\n",
    "# Image Metadata\n",
    "curl https://roman.ipac.caltech.edu/data/sims/sn_image_sims/paper_rotate.fits --output ${DATADIR}/paper_rotate.fits\n",
    "\n",
    "cd data; (for f in rotate_update_Y106_*.tar.gz; do tar xvzf $f \"*_1.fits.gz\"; done); gunzip *.fits.gz; cd -\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like you can view the FITS files and the SN region file (see below) with\n",
    "```\n",
    "ds9 data/rotate_update_Y106_132_1.fits data/rotate_update_Y106_174_1.fits data/rotate_update_Y106_175_1.fits -region load all sn.reg -scale mode zscale -scale match -frame lock wcs -zoom to fit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.time import Time\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "import astrophot as ap\n",
    "from astrophot.image import PSF_Image, Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SN and Host position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"DC2\":\n",
    "    # DC2\n",
    "    # fitting window will be a npix x npix box\n",
    "    npix_dict = {41021613806: 100,\n",
    "                 11392192729110: 100}\n",
    "\n",
    "    sn_dict = {\n",
    "        41021613806: {\"ra\": 60.2901401, \"dec\": -44.142051},\n",
    "        11392192729110: {\"ra\": 60.1996112, \"dec\": -44.2708990},\n",
    "    }\n",
    "    host_dict = {\n",
    "        41021613806: {\"ra\": 60.288242, \"dec\": -44.139890},\n",
    "        11392192729110: {\"ra\": 60.1996379, \"dec\": -44.2705822},\n",
    "    }\n",
    "\n",
    "    sn_id = 11392192729110\n",
    "    sn = sn_dict[sn_id]\n",
    "    host = host_dict[sn_id]\n",
    "    npix = npix_dict[sn_id]\n",
    "\n",
    "elif DATASET == \"RomanSN\":\n",
    "    # RomanSN\n",
    "    sn = {\"ra\": 71.30192566051916, \"dec\": -53.60051728973533}\n",
    "    # We don't have a separate galaxy position here\n",
    "    host = sn\n",
    "    npix = 50  # window will be a npix x npix box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_coord = SkyCoord(sn[\"ra\"], sn[\"dec\"], unit=u.degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"DC2\":\n",
    "    data_dir = \"data/DC2\"\n",
    "\n",
    "    image_file_basenames_dict = {\n",
    "        41021613806: [\n",
    "            \"image_calexp-g-944022-R43_S11-2025-10-14T04.fits\",\n",
    "            \"image_calexp-g-944052-R31_S12-2025-10-14T04.fits\",\n",
    "            \"image_calexp-g-944236-R31_S11-2025-10-14T06.fits\",\n",
    "            \"image_calexp-g-960109-R03_S21-2025-11-14T01.fits\",\n",
    "            \"image_calexp-g-975987-R43_S01-2025-12-08T02.fits\",\n",
    "            \"image_calexp-r-909835-R30_S01-2025-08-18T07.fits\",\n",
    "            \"image_calexp-r-909869-R31_S10-2025-08-18T08.fits\",\n",
    "            \"image_calexp-r-909956-R01_S02-2025-08-18T09.fits\",\n",
    "            \"image_calexp-r-910001-R12_S10-2025-08-18T09.fits\",\n",
    "            \"image_calexp-r-942690-R02_S11-2025-10-12T08.fits\",\n",
    "            \"image_calexp-r-942722-R23_S10-2025-10-12T08.fits\",\n",
    "            \"image_calexp-r-943370-R24_S12-2025-10-13T06.fits\",\n",
    "            \"image_calexp-r-943372-R22_S00-2025-10-13T06.fits\",\n",
    "            \"image_calexp-r-943428-R01_S22-2025-10-13T07.fits\",\n",
    "            \"image_calexp-r-963953-R21_S02-2025-11-19T02.fits\",\n",
    "            \"image_calexp-r-963987-R32_S20-2025-11-19T02.fits\",\n",
    "            \"image_calexp-r-964110-R03_S22-2025-11-19T04.fits\",\n",
    "            \"image_calexp-r-969979-R34_S10-2025-11-28T04.fits\",\n",
    "            \"image_calexp-r-970022-R24_S10-2025-11-28T04.fits\",\n",
    "            \"image_calexp-i-915668-R24_S22-2025-08-25T07.fits\",\n",
    "            \"image_calexp-i-915698-R01_S11-2025-08-25T07.fits\",\n",
    "            \"image_calexp-i-924121-R20_S20-2025-09-19T06.fits\",\n",
    "            \"image_calexp-i-934006-R31_S02-2025-10-01T06.fits\",\n",
    "            \"image_calexp-i-934015-R20_S12-2025-10-01T06.fits\",\n",
    "            \"image_calexp-i-941091-R11_S02-2025-10-10T08.fits\",\n",
    "            \"image_calexp-i-945601-R10_S12-2025-10-16T03.fits\",\n",
    "            \"image_calexp-i-966789-R01_S02-2025-11-24T01.fits\",\n",
    "            \"image_calexp-i-966821-R23_S10-2025-11-24T01.fits\",\n",
    "            \"image_calexp-i-966822-R24_S00-2025-11-24T01.fits\",\n",
    "            \"image_calexp-i-976120-R10_S11-2025-12-08T03.fits\",\n",
    "            \"image_calexp-i-976272-R33_S22-2025-12-08T05.fits\",\n",
    "            \"image_calexp-i-976304-R41_S00-2025-12-08T05.fits\",\n",
    "            \"image_calexp-i-976336-R23_S20-2025-12-08T06.fits\",\n",
    "        ],\n",
    "        11392192729110: [\n",
    "            \"image_calexp-r-896748-R14_S20-2025-08-03T08.fits\",\n",
    "            \"image_calexp-r-909835-R30_S01-2025-08-18T07.fits\",\n",
    "            \"image_calexp-r-963953-R21_S02-2025-11-19T02.fits\",\n",
    "            \"image_calexp-r-964110-R03_S22-2025-11-19T04.fits\",\n",
    "            \"image_calexp-r-969979-R34_S10-2025-11-28T04.fits\",\n",
    "            \"image_calexp-r-970022-R24_S10-2025-11-28T04.fits\",\n",
    "            \"image_calexp-i-941091-R11_S02-2025-10-10T08.fits\",\n",
    "            \"image_calexp-i-976272-R33_S22-2025-12-08T05.fits\",\n",
    "        ],\n",
    "    }\n",
    "    image_file_basenames = image_file_basenames_dict[sn_id]\n",
    "    # SN live from MJD 60930 to 61020\n",
    "\n",
    "    select_filter = re.compile(\"image_calexp-([a-zA-Z0-9]+)-\")\n",
    "    band = [select_filter.match(f).groups()[0] for f in image_file_basenames]\n",
    "\n",
    "    # Limit to only the \"r\" band for testing\n",
    "#    r_band, = np.where(np.array(band) == \"g\")\n",
    "#    image_file_basenames = np.array(image_file_basenames)[r_band]\n",
    "#    band = np.array(band)[r_band]\n",
    "\n",
    "elif DATASET == \"RomanSN\":\n",
    "    data_dir = \"data/RomanSN\"\n",
    "    band = \"Y106\"\n",
    "\n",
    "    # The metadata for the files is stored by row idx that is encoded in the filename\n",
    "    # We'll use that later to look up the informatino for the file.\n",
    "    image_info_row = [132, 174, 175]\n",
    "    detectors = [1, 1, 1]\n",
    "    image_file_basenames = [\n",
    "        f\"rotate_update_{band}_{idx}_{det}.fits\"\n",
    "        for idx, det in zip(image_info_row, detectors)\n",
    "    ]\n",
    "\n",
    "    band = len(image_file_basenames) * band\n",
    "\n",
    "else:\n",
    "    print(f\"Unsupported DATASET: {DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [os.path.join(data_dir, bn) for bn in image_file_basenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the PSF from the calexp PSFEx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_psf_2x2(psf):\n",
    "    nc, nx, ny = psf.shape\n",
    "    \n",
    "    # If odd, then we want to preserve the center row nx//2, nx//2\n",
    "    # And we'll average the other rows together, 2 at a time\n",
    "\n",
    "    # Rows\n",
    "    # nx = 40\n",
    "    # 0:20, 40-20:40\n",
    "    # \n",
    "    a = psf[:, :nx//2, :].reshape(nc, nx//4, 2, ny).sum(axis=2) / 2\n",
    "    b = psf[:, -(nx-1)//2:, :].reshape(nc, nx//4, 2, ny).sum(axis=2) / 2\n",
    "    if nx % 2 == 1:\n",
    "        x_pieces = [a, psf[:, nx//2:nx//2+1, :], b]\n",
    "    else:\n",
    "        x_pieces = [a, b]\n",
    "    c = np.concatenate(x_pieces, axis=1)\n",
    "    \n",
    "    # Columns\n",
    "    d = c[:, :, :ny//2].reshape(nc, (nx+1)//2, ny//4, 2).sum(axis=3) / 2\n",
    "    e = c[:, :, -(ny-1)//2:].reshape(nc, (nx+1)//2, ny//4, 2).sum(axis=3) / 2\n",
    "    if ny % 2 == 1:\n",
    "        y_pieces = [d, c[:, :, ny//2:ny//2+1], e]\n",
    "    else:\n",
    "        y_pieces = [d, e]\n",
    "    new_psf = np.concatenate(y_pieces, axis=2)\n",
    "    \n",
    "    return new_psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resample_psf_2x2_odd():\n",
    "    test_array_odd = np.zeros((6, 81, 81))\n",
    "    smaller_test_array_odd = resample_psf_2x2(test_array_odd)\n",
    "    assert np.shape(smaller_test_array_odd) == (6, 41, 41)\n",
    "\n",
    "def test_resample_psf_2x2_even():\n",
    "    test_array_even = np.zeros((6, 40, 40))\n",
    "    smaller_test_array_even = resample_psf_2x2(test_array_even)\n",
    "    assert np.shape(smaller_test_array_even) == (6, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resample_psf_2x2_odd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resample_psf_2x2_even()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_psfex_image(hdu_info, hdu_data, resample=False):\n",
    "#     group = hdu.data[\"group\"]\n",
    "#     degree = hdu.data[\"degree\"]\n",
    "#     basis = hdu.data[\"basis\"]\n",
    "#     coeff = hdu.data[\"coeff\"]\n",
    "    size = hdu_data.data[\"_size\"]\n",
    "    comp = hdu_data.data[\"_comp\"] \n",
    "#    print(hdu_data.data[\"_context_first\"])\n",
    "#    print(hdu_data.data[\"_context_second\"])\n",
    "\n",
    "    image = comp.reshape(*size[0][::-1])\n",
    "    \n",
    "    # The PSF is oversampled by pixstep\n",
    "    pixstep = hdu_info.data._pixstep[0]\n",
    "    \n",
    "    if resample:\n",
    "        image = resample_psf_2x2(image)\n",
    "    \n",
    "    return pixstep, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fwhm_from_psf_image(image):\n",
    "    _, nx, ny = np.shape(image)\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(nx) - nx//2, np.arange(ny) - ny//2)\n",
    "\n",
    "    norm = np.sum(image)\n",
    "\n",
    "    # Calculate first moment\n",
    "    x_center = np.sum(image * x**1) / norm\n",
    "    y_center = np.sum(image * y**1) / norm\n",
    "\n",
    "    # Calculate second moments\n",
    "    I_xx = (np.sum(image * (x-x_center)**2 * (y-y_center)**0) / norm)\n",
    "    I_xy = (np.sum(image * (x-x_center)**1 * (y-y_center)**1) / norm)\n",
    "    I_yy = (np.sum(image * (x-x_center)**0 * (y-y_center)**2) / norm)\n",
    "\n",
    "    fwhm = np.sqrt(I_xx + I_yy)\n",
    " \n",
    "    return fwhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = image_files[0]\n",
    "hdu = fits.open(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu[11].data._pixstep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixstep, image = read_psfex_image(hdu[11], hdu[12])\n",
    "fwhm = calc_fwhm_from_psf_image(image)\n",
    "print(fwhm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0, :, :], cmap=\"rainbow\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector, Image, and FITS file order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are 4k x 4k images\n",
    "pixel_scale = {\"DC2\": 0.2, \"RomanSN\": 0.11}  # \"/pixel\n",
    "fwhm = {\"DC2\": 0.6, \"RomanSN\": 0.2}  # \"\n",
    "\n",
    "# The HDU order is different between the two datasets\n",
    "HDU_IDX = {\n",
    "    \"DC2\": {\"image\": 1, \"mask\": 2, \"variance\": 3, \"psfex_info\": 11, \"psfex_data\": 12},\n",
    "    \"RomanSN\": {\"image\": 1, \"mask\": 3, \"variance\": 2},\n",
    "}\n",
    "# as are the FITS extension names\n",
    "HDU_NAMES = {\n",
    "    \"DC2\": {\"image\": \"image\", \"mask\": \"mask\", \"variance\": \"variance\"},\n",
    "    \"RomanSN\": {\"image\": \"SCI\", \"mask\": \"DQ\", \"variance\": \"ERR\"},\n",
    "}\n",
    "# so we have to use a translation regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bad pixel mask values\n",
    "bad_pixel_bitmask = {}\n",
    "\n",
    "## DC2\n",
    "# Pixel mask values are defined in\n",
    "# https://github.com/lsst/afw/blob/29afe694f19d80cba34b10ffd361dc6ca8d49dd1/src/image/detail/MaskDict.cc#L206\n",
    "# The \"right\" way is to use `getMaskPlaneDict` in the LSST Science Pipelines\n",
    "# but we don't want to introduce that dependency here.\n",
    "\n",
    "# \"EDGE\": 4 is not necessarily bad, although it could be a cause for concern we'll accept it for now\n",
    "# \"DETECTED\": 5 and \"DETECTED_NEGATIVE\": 6 are both bits indicating detection of objects\n",
    "# and are not \"bad\"\n",
    "basic_mask_plane_bits = {\n",
    "    \"BAD\": 0,\n",
    "    \"SAT\": 1,\n",
    "    \"INTRP\": 2,\n",
    "    \"CR\": 3,\n",
    "    \"SUSPECT\": 7,\n",
    "    \"NO_DATA\": 8,\n",
    "}\n",
    "bad_pixel_bitmask[\"DC2\"] = sum(\n",
    "    2 ** np.array([v for v in basic_mask_plane_bits.values()])\n",
    ")\n",
    "\n",
    "## Roman\n",
    "bad_pixel_bitmask[\"RomanSN\"] = 0b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image metadata table\n",
    "\n",
    "If you have the information, create a image metadata table here called `image_metadata`.  It will be used below to add key information to the AstroPhot target header metadata and which will in turn be used to build the lightcurve table for the photometry.  The table should have rows in the same order as the `image_file_basenames` (and `image_files`) arrays.  It is expected to have \"mjd\" and \"band\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DC2\n",
    "For DC2, the DATE-START, DATE-END, and DATA-AVG are stored in the header.  So we get the MJD by reading the headers and translate DATE-AVG to MJD\n",
    "\n",
    "#### RomanSN\n",
    "For Roman SN simulation, the dates are stored in a separate file: `paper_rotate.fits`.  We look up the MJD from appropriate row using the index in the simulated filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_truth = None\n",
    "\n",
    "if DATASET == \"DC2\":\n",
    "    mjd = []\n",
    "    for f in image_files:\n",
    "        header = fits.getheader(f)\n",
    "        dt = Time(header[\"DATE-AVG\"], scale=header[\"TIMESYS\"].lower())\n",
    "        mjd.append(dt.mjd)\n",
    "        \n",
    "    summary_file = os.path.join(data_dir, \"truth_sn_summary_v1-0-0.parquet\")\n",
    "    variability_file = os.path.join(data_dir, \"truth_sn_variability_v1-0-0.parquet\")\n",
    "\n",
    "    df = pd.read_parquet(variability_file)\n",
    "    lightcurve_truth = df[df[\"id\"] == sn_id]\n",
    "    \n",
    "    lightcurve_truth = Table.from_pandas(lightcurve_truth)\n",
    "\n",
    "    zp = 8.90 + 2.5 * 9\n",
    "    lightcurve_truth[\"mag\"] = -2.5 * np.log10(lightcurve_truth[\"delta_flux\"]) + zp\n",
    "    lightcurve_truth[\"band\"] = lightcurve_truth[\"bandpass\"]\n",
    "\n",
    "elif DATASET == \"RomanSN\":\n",
    "    image_metadata_basename = \"paper_rotate.fits\"\n",
    "    image_metadata_filename = os.path.join(data_dir, image_metadata_basename)\n",
    "    image_metadata = Table.read(image_metadata_filename)\n",
    "\n",
    "    mjd = image_metadata[image_info_row][\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "summary_file = os.path.join(data_dir, \"truth_sn_summary_v1-0-0.parquet\")\n",
    "df = pd.read_parquet(summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"id\"] == sn_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata = Table({\"image_filename\": image_file_basenames, \"mjd\": mjd})\n",
    "image_metadata[\"band\"] = band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a convenient region file of the SN coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out RA, Dec to prepare for making a ds9 region file.\n",
    "def write_ds9_region_file(\n",
    "    coordinate_table, region_filename=\"ds9.reg\", ra_colname=\"RA\", dec_colname=\"DEC\", id_colname=None,\n",
    "):\n",
    "    with open(region_filename, \"w\") as f:\n",
    "        f.write(\"wcs; icrs;\\n\")\n",
    "        if id_colname is None:\n",
    "            for r, d in coordinate_table[[ra_colname, dec_colname]]:\n",
    "                f.write(f\"point({r},{d})\\n\")\n",
    "        else:\n",
    "            for r, d, i in coordinate_table[[ra_colname, dec_colname, id_colname]]:\n",
    "                f.write(f\"point({r},{d})  # text={{{i}}}\\n\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"DC2\":\n",
    "    sn_metadata_basename = \"truth_sn_summary_v1-0-0.parquet\"\n",
    "    sn_metadata_filename = os.path.join(data_dir, sn_metadata_basename)\n",
    "    sn_metadata = Table.read(sn_metadata_filename)\n",
    "\n",
    "    min_mjd, max_mjd = 60888, 61040\n",
    "    in_date_range = (min_mjd < sn_metadata[\"t0\"]) & (sn_metadata[\"t0\"] < max_mjd)\n",
    "    mag_threshold = 24\n",
    "    in_mag_range = sn_metadata[\"mB\"] < mag_threshold \n",
    "    in_range = in_date_range & in_mag_range\n",
    "    \n",
    "    region_basename = \"dc2_sn.reg\"\n",
    "    region_filename = os.path.join(data_dir, region_basename)\n",
    "    \n",
    "    overwrite = False\n",
    "    if overwrite:\n",
    "        write_ds9_region_file(\n",
    "            coordinate_table=sn_metadata[in_range],\n",
    "            ra_colname=\"ra\",\n",
    "            dec_colname=\"dec\",\n",
    "            id_colname=\"id\",\n",
    "            region_filename=region_filename,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"RomanSN\":\n",
    "    sn_metadata_basename = \"WFIRST_AKARI_FIXED_HEAD.FITS\"\n",
    "    sn_metadata_filename = os.path.join(data_dir, sn_metadata_basename)\n",
    "    sn_metadata = Table.read(sn_metadata_filename)\n",
    "\n",
    "    overwrite = False\n",
    "    if overwrite:\n",
    "        write_ds9_region_file(coordinate_table=sn_metadata, region_filename=\"roman_sn.reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General SN+host fitting\n",
    "\n",
    "The rest of this Notebook should work in general for any data set (`image_files`), SN coordinates (`sn`), host coordinates (`host`), and lightcurve seeded with a column for MJD (`lightcurve`) set up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ZP = 22.5  # Appropriate if the image was calibrated and scaled to nanomaggies\n",
    "\n",
    "\n",
    "def make_target(\n",
    "    image_filepath,\n",
    "    coord: Optional[SkyCoord] = None,\n",
    "    fwhm: float = fwhm[DATASET],\n",
    "    psf_size: int = 51,\n",
    "    pixel_scale: float = pixel_scale[DATASET],\n",
    "    zeropoint: Optional[float] = None,\n",
    "    hdu_idx: dict = HDU_IDX[DATASET],\n",
    "    bad_pixel_bitmask: Optional[int] = bad_pixel_bitmask[DATASET],\n",
    "    do_mask=False,\n",
    "):\n",
    "    \"\"\"Make an AstroPhot target.\n",
    "\n",
    "    image_filepath: str, Filepath to image file.\n",
    "        Image file assumed to have [image, mask, variance].\n",
    "        WCS assumed to be present in image HDU header\n",
    "\n",
    "    coord: SkyCoord object with center of window\n",
    "    fwhm: float, Full-Width at Half-Maximum in arcsec\n",
    "    psf_size: float, width of the PSF\n",
    "    pixel_scale: float, \"/pix\n",
    "       This is used along with fwhm, psf_size to set a Gaussian PSF model\n",
    "       Would be better to have an actual PSF model from the image\n",
    "    pixel_shape: (int, int), pix\n",
    "    zeropoint: float, calibration of counts in image.\n",
    "    \"\"\"\n",
    "    hdu = fits.open(image_filepath)\n",
    "    header = hdu[0].header  # Primary header\n",
    "    img = hdu[hdu_idx[\"image\"]].data  # Image HDU\n",
    "    var = hdu[hdu_idx[\"variance\"]].data  # Variance HDU\n",
    "\n",
    "    sigma_to_fwhm = 2.355\n",
    "\n",
    "    if do_mask:\n",
    "        # But need to translate the informative mask with a bad-pixel mask.\n",
    "        # E.g., for an LSST Science Pipelines mask, one of the mask values\n",
    "        # is that that pixel is part of a footprint of a valid object\n",
    "        # We don't want to mask those!\n",
    "        informative_mask = hdu[hdu_idx[\"mask\"]].data  # Mask\n",
    "        bad_pixel_mask = informative_mask & bad_pixel_bitmask\n",
    "\n",
    "    # LSST Science Pipelines processed data will store a zeropoint in MAGZERO\n",
    "    if zeropoint is None:\n",
    "        try:\n",
    "            zeropoint = header[\"MAGZERO\"] + 2.5 * np.log10(header[\"EXPTIME\"])\n",
    "        except:\n",
    "            zeropoint = DEFAULT_ZP\n",
    "\n",
    "    wcs = WCS(hdu[hdu_idx[\"image\"]].header)\n",
    "\n",
    "    # If a PSF image is available, use it to calculate FWHM\n",
    "    if \"psfex_info\" in hdu_idx.keys():\n",
    "        pixstep, image = read_psfex_image(\n",
    "            hdu[hdu_idx[\"psfex_info\"]], hdu[hdu_idx[\"psfex_data\"]], resample=True,\n",
    "        )\n",
    "        pixel_scale = 3600 * wcs.pixel_scale_matrix\n",
    "\n",
    "        psf_upscale = round(1 / pixstep)\n",
    "        # Tensor expects float64\n",
    "        psf = image[0, :, :].astype(\"float64\")  # just take main component\n",
    "        psf /= np.sum(psf)\n",
    "#         print(psf_upscale)\n",
    "#         psf = PSF_Image(\n",
    "#             data=psf,\n",
    "#             psf_upscale=psf_upscale,\n",
    "#             pixelscale=pixel_scale,\n",
    "#         )\n",
    "        fwhm = calc_fwhm_from_psf_image(image)\n",
    "        print(fwhm * pixel_scale * pixstep)\n",
    "    else:\n",
    "        # we construct a basic gaussian psf for each image\n",
    "        # by giving the simga (arcsec), image width (pixels), and pixelscale (arcsec/pixel)\n",
    "        psf = ap.utils.initialize.gaussian_psf(\n",
    "            fwhm / sigma_to_fwhm, psf_size, pixel_scale\n",
    "        )\n",
    "\n",
    "    target_kwargs = {\n",
    "        \"data\": np.array(img, dtype=np.float64),\n",
    "        \"variance\": var,\n",
    "        \"zeropoint\": zeropoint,\n",
    "        \"psf\": psf,\n",
    "        \"wcs\": wcs,\n",
    "    }\n",
    "\n",
    "    if do_mask:\n",
    "        target_kwargs[\"mask\"] = bad_pixel_mask\n",
    "    if coord is not None:\n",
    "        target_kwargs[\"reference_radec\"] = (coord.ra.degree, coord.dec.degree)\n",
    "\n",
    "    target = ap.image.Target_Image(**target_kwargs)\n",
    "\n",
    "    target.header.filename = image_filepath\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ap.image.Target_Image_List(make_target(f, coord=sn_coord) for f in image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add MJD and band from lightcurve table to the target metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume targets and lightcurve are in same order.\n",
    "# Could do a more robust lookup by lightcurve filename, but not implemented for now\n",
    "for i, r in enumerate(image_metadata):\n",
    "    targets[i].header.mjd = r[\"mjd\"]\n",
    "    targets[i].header.band = r[\"band\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot just the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_for_target(target, ra, dec, npix=npix):\n",
    "    window = target.window.copy()\n",
    "    center_xy = window.world_to_pixel(ra, dec)\n",
    "\n",
    "    xmin = center_xy[0] - npix // 2\n",
    "    xmax = center_xy[0] + npix // 2\n",
    "    ymin = center_xy[1] - npix // 2\n",
    "    ymax = center_xy[1] + npix // 2\n",
    "\n",
    "    window.crop_to_pixel([[xmin, xmax], [ymin, ymax]])\n",
    "    return window\n",
    "\n",
    "def make_windows_for_targets(targets, ra, dec, npix=npix):\n",
    "    windows = [make_window_for_target(t, ra, dec) for t in targets]\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = make_windows_for_targets(targets, sn[\"ra\"], sn[\"dec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(targets.image_list)\n",
    "side = int(np.sqrt(n)) + 1\n",
    "fig, ax = plt.subplots(side, side, figsize=(3 * side, 3 * side))\n",
    "\n",
    "for i in range(n):\n",
    "    ap.plots.target_image(fig, ax.ravel()[i], targets[i], window=windows[i], flipx=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinate axes are in arcseconds, but in the local relative coordinate system for each image.  AstroPhot used the pixel scale to translate pixels -> arcsec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate SN and host positions to projection plane positions for target.  By construction of our targets, this is in the same projection plane position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_xy = targets[0].world_to_plane(sn[\"ra\"], sn[\"dec\"])\n",
    "host_xy = targets[0].world_to_plane(host[\"ra\"], host[\"dec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide up because \"model_image\" expects a single axis object if single image\n",
    "# while it wants an array of axis objects if there are multiple images in the image list\n",
    "# model_image will not accept a one-element array if there is no image_list\n",
    "def plot_target_model(model, **kwargs):\n",
    "    if hasattr(model.target, \"image_list\"):\n",
    "        _plot_target_model_multiple(model, **kwargs)\n",
    "    else:\n",
    "        _plot_target_model_single(model, **kwargs)\n",
    "\n",
    "\n",
    "def _plot_target_model_multiple(model, window=None, titles=None, base_figsize=(12, 4), figsize=None):\n",
    "    n = len(model.target.image_list)\n",
    "    if figsize is None:\n",
    "        figsize = (base_figsize[0], n*base_figsize[1])\n",
    "    fig, ax = plt.subplots(n, 3, figsize=figsize)\n",
    "    # Would like to just call this, but window isn't parsed as a list\n",
    "    # https://github.com/Autostronomy/AstroPhot/issues/142\n",
    "    #    ap.plots.target_image(fig, ax[:, 0], model.target, window=window, flipx=True)\n",
    "    for axt, mod, win in zip(ax[:, 0], model.target.image_list, window):\n",
    "        ap.plots.target_image(fig, axt, mod, win, flipx=True)\n",
    "\n",
    "    if titles is not None:\n",
    "        for i, title in enumerate(titles):\n",
    "            ax[i, 0].set_title(title)\n",
    "    ap.plots.model_image(fig, ax[:, 1], model, window=window, flipx=True)\n",
    "    ax[0, 1].set_title(\"Model\")\n",
    "    ap.plots.residual_image(fig, ax[:, 2], model, window=window, flipx=True)\n",
    "    ax[0, 2].set_title(\"Residual\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _plot_target_model_single(model, window=None, title=None, figsize=(16, 4)):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    ap.plots.target_image(fig, ax[0], model.target, window=window, flipx=True)\n",
    "    ax[0].set_title(title)\n",
    "    ap.plots.model_image(fig, ax[1], model, window=window, flipx=True)\n",
    "    ax[1].set_title(\"Model\")\n",
    "    ap.plots.residual_image(fig, ax[2], model, window=window, flipx=True)\n",
    "    ax[2].set_title(\"Residual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointly fit model across images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sky = []\n",
    "model_host = []\n",
    "model_sn = []\n",
    "\n",
    "# The DC2 images are calibrated exposures that have the sky subtracted\n",
    "# The RomanSN images are \"raw\" science images with sky.\n",
    "FIT_SKY = {\"DC2\": False, \"RomanSN\": True}\n",
    "FIT_HOST = True\n",
    "FIT_SN = True\n",
    "CORRECT_SIP = True\n",
    "\n",
    "if FIT_SKY[DATASET]:\n",
    "    for i, (target, window) in enumerate(zip(targets, windows)):\n",
    "        model_sky.append(\n",
    "            ap.models.AstroPhot_Model(\n",
    "                name=f\"sky model {i}\",\n",
    "                model_type=\"flat sky model\",\n",
    "                target=target,\n",
    "                window=window,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "if FIT_HOST:\n",
    "    model_host_band = {}\n",
    "    for i, (b, target, window) in enumerate(zip(band, targets, windows)):\n",
    "        model_host.append(\n",
    "            ap.models.AstroPhot_Model(\n",
    "                name=f\"host model {i}\",\n",
    "                model_type=\"sersic galaxy model\",\n",
    "                target=target,\n",
    "                psf_mode=\"full\",\n",
    "                parameters={\"center\": host_xy},\n",
    "                window=window,\n",
    "            )\n",
    "        )\n",
    "        # I think this assignment copies reference that points to same underlying object\n",
    "        # in 'model_host' and 'model_host_band'\n",
    "        # The initialization step assumes that the reference model gets initialized first.\n",
    "        # So we just mark use the first model in the list of each band.\n",
    "        if b not in model_host_band.keys():\n",
    "            model_host_band[b] = i  \n",
    "        \n",
    "if FIT_SN:\n",
    "    for i, (target, window) in enumerate(zip(targets, windows)):\n",
    "        model_sn.append(\n",
    "            ap.models.AstroPhot_Model(\n",
    "                name=f\"SN model {i}\",\n",
    "                model_type=\"psf star model\",\n",
    "                psf=target.psf,\n",
    "                target=target,\n",
    "                psf_mode=\"none\",\n",
    "                parameters={\"center\": sn_xy},\n",
    "                window=window,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AstroPhot doesn't handle SIP WCS yet.\n",
    "# We'll roughly work around this by allowing a small shift in position\n",
    "# for all (both) objects on the image.\n",
    "CORRECT_SIP = True\n",
    "if CORRECT_SIP:\n",
    "    def calc_center(params):\n",
    "        return params[\"nominal_center\"].value + params[\"astrometric\"].value\n",
    "\n",
    "    if FIT_HOST and FIT_SN:\n",
    "        host_center = ap.param.Parameter_Node(\n",
    "            name = \"nominal_center\",\n",
    "            value = host_xy    \n",
    "        )\n",
    "\n",
    "        sn_center = ap.param.Parameter_Node(\n",
    "            name = \"nominal_center\",\n",
    "            value = sn_xy\n",
    "        )\n",
    "            \n",
    "        for i in range(len(model_host)):\n",
    "            # The x, y delta is the same for both the SN and host\n",
    "            # but can be different for each image.\n",
    "            P_astrometric = ap.param.Parameter_Node(\n",
    "                name = \"astrometric\",\n",
    "                value = [0, 0],\n",
    "            )\n",
    "\n",
    "            model_host[i][\"center\"].value = calc_center\n",
    "            model_host[i][\"center\"].link(host_center, P_astrometric)\n",
    "            \n",
    "            model_sn[i][\"center\"].value = calc_center\n",
    "            model_sn[i][\"center\"].link(sn_center, P_astrometric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrain host model to be the same per band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, model in zip(band, model_host):\n",
    "    if model.name == model_host[model_host_band[b]].name:\n",
    "        continue\n",
    "    for parameter in [\"q\", \"PA\", \"n\", \"Re\", \"Ie\"]:\n",
    "        model[parameter].value = model_host[model_host_band[b]][parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a two-tier hierarchy of group models\n",
    "# following recommendation from Connor Stone.\n",
    "\n",
    "# Group model for each class: sky, host, sn\n",
    "all_model_list = []\n",
    "if len(model_sky) > 0:\n",
    "    sky_group_model = ap.models.AstroPhot_Model(\n",
    "        name=\"Sky\",\n",
    "        model_type=\"group model\",\n",
    "        models=[*model_sky],\n",
    "        target=targets,\n",
    "    )\n",
    "    all_model_list.extend(sky_group_model)\n",
    "\n",
    "if len(model_host) > 0:\n",
    "    host_group_model = ap.models.AstroPhot_Model(\n",
    "        name=\"Host\",\n",
    "        model_type=\"group model\",\n",
    "        models=[*model_host],\n",
    "        target=targets,\n",
    "    )\n",
    "    all_model_list.extend(host_group_model)\n",
    "\n",
    "if len(model_sn) > 0:\n",
    "    sn_group_model = ap.models.AstroPhot_Model(\n",
    "        name=\"SN\",\n",
    "        model_type=\"group model\",\n",
    "        models=[*model_sn],\n",
    "        target=targets,\n",
    "    )\n",
    "    all_model_list.extend(sn_group_model)\n",
    "\n",
    "# Group model holds all the classes\n",
    "model_host_sn = ap.models.AstroPhot_Model(\n",
    "    name=\"Host+SN\",\n",
    "    model_type=\"group model\",\n",
    "    models=all_model_list,\n",
    "    target=targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to initialize the model so that there is a value for `parameters[\"center\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_host_sn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_host_sn.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ap.fit.LM(model_host_sn, verbose=True).fit()\n",
    "print(result.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainties above aren't actually particularly real uncertainties.  They're from the initial uncertainty set when initializing the object.  To update the uncertainty, we can use the coraviance matrix.  We'll just take the diagonal for now.\n",
    "\n",
    "But there could be noticeable off-diagonal elements. E.g., the flux estimates are [anti]correlated with the galaxy estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.update_uncertainty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainties for the center positions and astrometric uncertainties aren't calculated correctly right now.\n",
    "\n",
    "But the the flux uncertainties are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = result.covariance_matrix.detach().cpu().numpy()\n",
    "plt.imshow(\n",
    "    covar,\n",
    "    origin=\"lower\",\n",
    "    vmin=1e-8, vmax=1e-1, norm=\"log\",\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the SN flux uncertainties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little clunky because I don't have a better way of looking up the names of the parameters in the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_flux_starts_at_parameter_idx = -len(targets.image_list)\n",
    "covar = result.covariance_matrix.detach().cpu().numpy()\n",
    "plt.imshow(\n",
    "    covar[sn_flux_starts_at_parameter_idx:, sn_flux_starts_at_parameter_idx:],\n",
    "    origin=\"lower\",\n",
    "    vmin=1e-6, vmax=1, norm=\"log\",\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_model_names = [f\"SN model {i}\" for i in range(len(targets.image_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [model_host_sn.models[m].target.header.filename for m in sn_model_names]\n",
    "bands = [model_host_sn.models[m].target.header.band for m in sn_model_names]\n",
    "mag = [\n",
    "    -2.5 * model_host_sn.models[m].parameters[\"flux\"].value.detach().cpu().numpy()\n",
    "    + model_host_sn.models[m].target.zeropoint.detach().cpu().numpy()\n",
    "    for m in sn_model_names\n",
    "]\n",
    "# mag_err = 2.5 * diagonal_parameter_std[sn_flux_starts_at_parameter_idx:]\n",
    "mag_err = [\n",
    "    2.5 * model_host_sn.models[m].parameters[\"flux\"].uncertainty.detach().cpu().numpy() for m in sn_model_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model_host_sn.models[m].target.zeropoint.detach().cpu().numpy() for m in sn_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve = Table(\n",
    "    {\"filename\": filenames, \"band\": bands, \"mjd\": mjd, \"mag\": mag, \"mag_err\": mag_err}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve[\"mjd\"].info.format = \">10.3f\"\n",
    "lightcurve[\"mag\"].info.format = \">7.4f\"\n",
    "lightcurve[\"mag_err\"].info.format = \">7.4f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model is in log10flux instead of linear flux, so non-detections appear with very large mag_err."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_for_band = {\n",
    "    \"u\": \"purple\",\n",
    "    \"g\": \"blue\",\n",
    "    \"r\": \"green\",\n",
    "    \"i\": \"red\",\n",
    "    \"z\": \"black\",\n",
    "    \"y\": \"yellow\",\n",
    "}\n",
    "\n",
    "# Don't print non-detections until we switch to flux fitting\n",
    "mag_err_threshold = 0.5\n",
    "for b in np.unique(lightcurve[\"band\"]):\n",
    "    (idx,) = np.where(\n",
    "        (lightcurve[\"band\"] == b) & (lightcurve[\"mag_err\"] < mag_err_threshold)\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        lightcurve[idx][\"mjd\"],\n",
    "        lightcurve[idx][\"mag\"] + 3.7,\n",
    "        lightcurve[idx][\"mag_err\"],\n",
    "        marker=\"o\",\n",
    "        markerfacecolor=color_for_band[b],\n",
    "        markeredgecolor=color_for_band[b],\n",
    "        ecolor=color_for_band[b],\n",
    "        linestyle=\"none\",\n",
    "        label=f\"{b}\",\n",
    "    )\n",
    "plt.ylabel(\"mag\")\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.title(f\"Proof of Concept: {DATASET}\")\n",
    "# plt.ylim(23.5, 17)\n",
    "\n",
    "\n",
    "if lightcurve_truth is not None:\n",
    "    for b in np.unique(lightcurve[\"band\"]):\n",
    "        (idx,) = np.where(lightcurve_truth[\"band\"] == b)\n",
    "        plt.scatter(\n",
    "            lightcurve_truth[idx][\"MJD\"],\n",
    "            lightcurve_truth[idx][\"mag\"],\n",
    "            color=color_for_band[b],\n",
    "            marker=\"*\",\n",
    "            label=f\"model {b}\",\n",
    "        )\n",
    "        \n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(plt.ylim()[::-1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_truth[lightcurve_truth[\"band\"] == \"r\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current spread in values during a night is a very loose guide to the amount we need to improve in extracting the lightcurve in this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_target_model(\n",
    "    model_host_sn,\n",
    "    window=windows,\n",
    "    titles=image_file_basenames,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
